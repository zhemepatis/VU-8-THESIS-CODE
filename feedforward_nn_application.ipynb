{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fecf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from utils.data_generation import generate_vectors, generate_scalars\n",
    "from benchmark_functions.sphere import sphere_func\n",
    "from neural_network_models.feedforward_nn import FeedforwardNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac78adc",
   "metadata": {},
   "source": [
    "# Generating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d533119",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1000)\n",
    "torch.manual_seed(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55bafa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data generation configuration\n",
    "input_dimention = 4\n",
    "domain = [-5, 5]\n",
    "\n",
    "# noise configuration\n",
    "add_noise = True\n",
    "noise_mean = 0\n",
    "noise_std_deviation = 0.5\n",
    "\n",
    "# data set configuration\n",
    "data_set_size = 10_000\n",
    "training_set_fraction = 0.7\n",
    "validation_set_fraction = 0.15\n",
    "test_set_fraction = 0.15\n",
    "\n",
    "# model configuration\n",
    "h1_neuron_number = 70\n",
    "output_neuron_number = 1\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "# training configuration\n",
    "batch_size = 8\n",
    "delta = 1e-6\n",
    "epoch_limit = 100\n",
    "patience_limit = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72e4562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate training data\n",
    "data_set_vectors = generate_vectors(input_dimention, domain, data_set_size)\n",
    "data_set_scalars = generate_scalars(data_set_vectors, sphere_func)\n",
    "\n",
    "# separate training data from validation and test data\n",
    "training_set_vectors, temp_set_vectors, training_set_scalars, temp_set_scalars = train_test_split(\n",
    "    data_set_vectors, \n",
    "    data_set_scalars, \n",
    "    test_size = (validation_set_fraction + test_set_fraction), \n",
    "    random_state = 42\n",
    ")\n",
    "\n",
    "# separate validation data from test data\n",
    "validation_set_vectors, test_set_vectors, validation_set_scalars, test_set_scalars = train_test_split(\n",
    "    temp_set_vectors, \n",
    "    temp_set_scalars, \n",
    "    test_size = (test_set_fraction / (test_set_fraction + validation_set_fraction)), \n",
    "    random_state = 42\n",
    ")\n",
    "\n",
    "# generate noise\n",
    "training_noise = []\n",
    "validation_noise = []\n",
    "\n",
    "if add_noise:\n",
    "    training_noise = np.random.normal(noise_mean, noise_std_deviation, len(training_set_scalars))\n",
    "    validation_noise = np.random.normal(noise_mean, noise_std_deviation, len(validation_set_scalars))\n",
    "\n",
    "    training_set_scalars += training_noise\n",
    "    validation_set_scalars += validation_noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ff4d59",
   "metadata": {},
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752c47e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_scaler = MinMaxScaler().fit(training_set_vectors)\n",
    "scalar_scaler = MinMaxScaler().fit(training_set_scalars.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba6a022",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_vectors = vector_scaler.transform(training_set_vectors)\n",
    "validation_set_vectors = vector_scaler.transform(validation_set_vectors)\n",
    "test_set_vectors = vector_scaler.transform(test_set_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9e20df",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_scalars = scalar_scaler.transform(training_set_scalars.reshape(-1, 1))\n",
    "validation_set_scalars = scalar_scaler.transform(validation_set_scalars.reshape(-1, 1))\n",
    "test_set_scalars = scalar_scaler.transform(test_set_scalars.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1133dd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert generated data to tensors\n",
    "training_set_vectors = torch.FloatTensor(training_set_vectors)\n",
    "training_set_scalars = torch.FloatTensor(training_set_scalars)\n",
    "\n",
    "validation_set_vectors = torch.FloatTensor(validation_set_vectors)\n",
    "validation_set_scalars = torch.FloatTensor(validation_set_scalars)\n",
    "\n",
    "test_set_vectors = torch.FloatTensor(test_set_vectors)\n",
    "test_set_scalars = torch.FloatTensor(test_set_scalars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810f78c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine generated data into tensor datasets\n",
    "train_dataset = TensorDataset(training_set_vectors, training_set_scalars)\n",
    "val_dataset = TensorDataset(validation_set_vectors, validation_set_scalars)\n",
    "test_dataset = TensorDataset(test_set_vectors, test_set_scalars)\n",
    "\n",
    "# create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "val_loader = DataLoader(val_dataset, batch_size = batch_size, shuffle = False)\n",
    "test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a95b99f",
   "metadata": {},
   "source": [
    "# Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236e2529",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FeedforwardNN(input_neuron_num = input_dimention, h1_neuron_num = h1_neuron_number, output_neuron_num = output_neuron_number)\n",
    "\n",
    "loss_func = nn.MSELoss()\n",
    "loss_optimization_func = optim.Adam(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f6feef",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_losses = []\n",
    "validation_losses = []\n",
    "\n",
    "best_validation_loss = float(\"inf\")\n",
    "best_model_state = None\n",
    "\n",
    "patience_tries = 0\n",
    "\n",
    "for epoch in range(epoch_limit):\n",
    "    # training step\n",
    "    model.train()\n",
    "    epoch_training_loss = 0.0\n",
    "\n",
    "    for batch_vectors, batch_scalars in train_loader:\n",
    "        # pass forward\n",
    "        predictions = model(batch_vectors)\n",
    "        loss = loss_func(predictions, batch_scalars)\n",
    "        \n",
    "        # back-propagation\n",
    "        loss_optimization_func.zero_grad()\n",
    "        loss.backward()\n",
    "        loss_optimization_func.step()\n",
    "        \n",
    "        epoch_training_loss += loss.item() * batch_vectors.size(0)\n",
    "\n",
    "    # average batch loss\n",
    "    epoch_training_loss /= len(train_loader.dataset)\n",
    "    training_losses.append(epoch_training_loss)\n",
    "\n",
    "    # validation step\n",
    "    model.eval()\n",
    "    epoch_validation_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_vectors, batch_scalars in val_loader:\n",
    "            predictions = model(batch_vectors)\n",
    "            loss = loss_func(predictions, batch_scalars)\n",
    "            epoch_validation_loss += loss.item() * batch_vectors.size(0)\n",
    "\n",
    "    epoch_validation_loss /= len(val_loader.dataset)\n",
    "    validation_losses.append(epoch_validation_loss)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"[{epoch + 1}] Training loss: {epoch_training_loss:.6f} | Validation loss: {epoch_validation_loss:.6f}\")\n",
    "\n",
    "    # stop condition\n",
    "    if epoch_validation_loss < best_validation_loss - delta:\n",
    "        best_validation_loss = epoch_validation_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        patience_tries = 0\n",
    "    else:\n",
    "        patience_tries += 1\n",
    "\n",
    "        if patience_tries >= patience_limit:\n",
    "            print(f\"Early stopping at epoch {epoch + 1} (no improvement for {patience_limit} epochs).\")\n",
    "            break\n",
    "\n",
    "# load best model\n",
    "model.load_state_dict(best_model_state)\n",
    "print(f\"Loaded best model with validation loss: {best_validation_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee463908",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8, 6))\n",
    "\n",
    "plt.plot(training_losses, label = 'Training Loss')\n",
    "plt.plot(validation_losses, label = 'Validation Loss')\n",
    "\n",
    "plt.title('Training vs Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8198a735",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_set_predictions = model(test_set_vectors)\n",
    "    test_loss = loss_func(test_set_predictions, test_set_scalars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c0d526",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_predictions = test_set_predictions.numpy()\n",
    "test_set_scalars = test_set_scalars.numpy()\n",
    "\n",
    "test_set_predictions = scalar_scaler.inverse_transform(test_set_predictions)\n",
    "test_set_scalars = scalar_scaler.inverse_transform(test_set_scalars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f20793",
   "metadata": {},
   "outputs": [],
   "source": [
    "absolute_errors = np.abs(test_set_predictions - test_set_scalars)\n",
    "\n",
    "# min\n",
    "min_error = np.min(absolute_errors)\n",
    "print(f\"Test set absolute error min: {min_error:.6f}\")\n",
    "\n",
    "# max\n",
    "max_error = np.max(absolute_errors)\n",
    "print(f\"Test set absolute error max: {max_error:.6f}\")\n",
    "\n",
    "# mean\n",
    "error_mean = np.mean(absolute_errors)\n",
    "print(f\"Test set absolute error mean: {error_mean:.6f}\")\n",
    "\n",
    "# standard deviation\n",
    "error_std = np.std(absolute_errors)\n",
    "print(f\"Test set absolute error standard deviation: {error_std:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.14.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
