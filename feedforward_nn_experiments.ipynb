{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9b80c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs.experiment_config import ExperimentConfig\n",
    "from configs.data_set_config import DataSetConfig\n",
    "from configs.noise_config import NoiseConfig\n",
    "from configs.training_config import TrainingConfig\n",
    "from configs.feedforward_nn_config import FeedforwardNNConfig\n",
    "\n",
    "from experiment_runners.feedforward_nn_runner import FeedforwardNNRunner\n",
    "\n",
    "from utils.benchmark_functions import sphere_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2651b25",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ExperimentConfig.__init__() got an unexpected keyword argument 'verbose'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m experiment_config = \u001b[43mExperimentConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtry_count\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1_000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m      4\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# data set configuration\u001b[39;00m\n\u001b[32m      7\u001b[39m data_set_config = DataSetConfig(\n\u001b[32m      8\u001b[39m     \u001b[38;5;66;03m# data generation configuration\u001b[39;00m\n\u001b[32m      9\u001b[39m     benchmark_function = sphere_func,\n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m     test_set_fraction = \u001b[32m0.15\u001b[39m\n\u001b[32m     17\u001b[39m )\n",
      "\u001b[31mTypeError\u001b[39m: ExperimentConfig.__init__() got an unexpected keyword argument 'verbose'"
     ]
    }
   ],
   "source": [
    "experiment_config = ExperimentConfig(\n",
    "    try_count = 1_000,\n",
    "    verbose = False\n",
    ")\n",
    "\n",
    "# data set configuration\n",
    "data_set_config = DataSetConfig(\n",
    "    # data generation configuration\n",
    "    benchmark_function = sphere_func,\n",
    "    input_dimension = 4,\n",
    "    component_domain = [-5, 5],\n",
    "    data_set_size = 1_000,\n",
    "    # data split configuration\n",
    "    training_set_fraction = 0.7,\n",
    "    validation_set_fraction = 0.15,\n",
    "    test_set_fraction = 0.15\n",
    ")\n",
    "\n",
    "noise_config = NoiseConfig(\n",
    "    mean = 0,\n",
    "    std = 0.5\n",
    ")\n",
    "\n",
    "# artificial neural netwokr model configuration \n",
    "fnn_config :FeedforwardNNConfig = FeedforwardNNConfig(\n",
    "    input_neuron_num = data_set_config.input_dimension,\n",
    "    h1_neuron_num = 70,\n",
    "    output_neuron_num = 1\n",
    ")\n",
    "\n",
    "training_config :TrainingConfig = TrainingConfig(\n",
    "    batch_size = 8,\n",
    "    delta = 1e-6,\n",
    "    epoch_limit = 100,\n",
    "    patience_limit = 13,\n",
    "    learning_rate = 0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb1cf36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10] Training loss: 0.036004 | Validation loss: 0.038182\n",
      "[20] Training loss: 0.023198 | Validation loss: 0.022635\n",
      "[30] Training loss: 0.003267 | Validation loss: 0.006859\n",
      "[40] Training loss: 0.002016 | Validation loss: 0.001962\n",
      "[50] Training loss: 0.001988 | Validation loss: 0.001757\n",
      "[60] Training loss: 0.001360 | Validation loss: 0.001659\n",
      "Early stopping at epoch 69 (no improvement for 13 epochs).\n",
      "Loaded best model with validation loss: 0.001235\n",
      "[10] Training loss: 0.036088 | Validation loss: 0.035969\n",
      "[20] Training loss: 0.007525 | Validation loss: 0.008422\n",
      "[30] Training loss: 0.002325 | Validation loss: 0.001901\n",
      "[40] Training loss: 0.001584 | Validation loss: 0.001863\n",
      "Early stopping at epoch 49 (no improvement for 13 epochs).\n",
      "Loaded best model with validation loss: 0.001389\n",
      "[10] Training loss: 0.033647 | Validation loss: 0.029812\n",
      "[20] Training loss: 0.009571 | Validation loss: 0.010316\n",
      "[30] Training loss: 0.002107 | Validation loss: 0.001838\n",
      "[40] Training loss: 0.001975 | Validation loss: 0.001798\n",
      "[50] Training loss: 0.001813 | Validation loss: 0.003102\n",
      "[60] Training loss: 0.001342 | Validation loss: 0.004104\n",
      "[70] Training loss: 0.001118 | Validation loss: 0.002142\n",
      "[80] Training loss: 0.000860 | Validation loss: 0.001812\n",
      "[90] Training loss: 0.001193 | Validation loss: 0.000991\n",
      "[100] Training loss: 0.000984 | Validation loss: 0.002181\n",
      "Loaded best model with validation loss: 0.000641\n",
      "[10] Training loss: 0.028583 | Validation loss: 0.026449\n",
      "[20] Training loss: 0.008283 | Validation loss: 0.008973\n",
      "[30] Training loss: 0.002111 | Validation loss: 0.001424\n",
      "[40] Training loss: 0.001635 | Validation loss: 0.001035\n",
      "[50] Training loss: 0.001212 | Validation loss: 0.001214\n",
      "Early stopping at epoch 53 (no improvement for 13 epochs).\n",
      "Loaded best model with validation loss: 0.001035\n",
      "[10] Training loss: 0.031190 | Validation loss: 0.031536\n",
      "[20] Training loss: 0.013021 | Validation loss: 0.014952\n",
      "[30] Training loss: 0.002016 | Validation loss: 0.003277\n",
      "[40] Training loss: 0.001500 | Validation loss: 0.002229\n",
      "[50] Training loss: 0.001446 | Validation loss: 0.001491\n",
      "[60] Training loss: 0.001455 | Validation loss: 0.000841\n",
      "[70] Training loss: 0.000994 | Validation loss: 0.000685\n",
      "[80] Training loss: 0.001331 | Validation loss: 0.002580\n",
      "Early stopping at epoch 83 (no improvement for 13 epochs).\n",
      "Loaded best model with validation loss: 0.000685\n",
      "[10] Training loss: 0.040280 | Validation loss: 0.033070\n",
      "[20] Training loss: 0.028325 | Validation loss: 0.028724\n",
      "[30] Training loss: 0.003947 | Validation loss: 0.003574\n",
      "[40] Training loss: 0.001902 | Validation loss: 0.003232\n",
      "[50] Training loss: 0.001600 | Validation loss: 0.001956\n",
      "[60] Training loss: 0.002455 | Validation loss: 0.001804\n",
      "[70] Training loss: 0.001267 | Validation loss: 0.001565\n",
      "[80] Training loss: 0.001231 | Validation loss: 0.001354\n",
      "[90] Training loss: 0.000901 | Validation loss: 0.001243\n",
      "[100] Training loss: 0.001079 | Validation loss: 0.001122\n",
      "Loaded best model with validation loss: 0.000709\n",
      "[10] Training loss: 0.037838 | Validation loss: 0.043514\n",
      "[20] Training loss: 0.018162 | Validation loss: 0.019662\n",
      "[30] Training loss: 0.002454 | Validation loss: 0.002610\n",
      "[40] Training loss: 0.001841 | Validation loss: 0.002084\n",
      "[50] Training loss: 0.001496 | Validation loss: 0.001174\n",
      "[60] Training loss: 0.001228 | Validation loss: 0.002352\n",
      "[70] Training loss: 0.001498 | Validation loss: 0.001509\n",
      "[80] Training loss: 0.001294 | Validation loss: 0.003922\n",
      "[90] Training loss: 0.001342 | Validation loss: 0.001408\n",
      "[100] Training loss: 0.000972 | Validation loss: 0.000823\n",
      "Early stopping at epoch 100 (no improvement for 13 epochs).\n",
      "Loaded best model with validation loss: 0.000768\n",
      "[10] Training loss: 0.030796 | Validation loss: 0.041949\n",
      "[20] Training loss: 0.020853 | Validation loss: 0.023191\n",
      "[30] Training loss: 0.003553 | Validation loss: 0.002456\n",
      "[40] Training loss: 0.001934 | Validation loss: 0.001154\n",
      "[50] Training loss: 0.001323 | Validation loss: 0.001088\n",
      "[60] Training loss: 0.001336 | Validation loss: 0.000745\n",
      "[70] Training loss: 0.001087 | Validation loss: 0.001753\n",
      "[80] Training loss: 0.001366 | Validation loss: 0.000878\n",
      "[90] Training loss: 0.001919 | Validation loss: 0.000828\n",
      "[100] Training loss: 0.001003 | Validation loss: 0.001148\n",
      "Loaded best model with validation loss: 0.000602\n",
      "[10] Training loss: 0.037819 | Validation loss: 0.036224\n",
      "[20] Training loss: 0.009512 | Validation loss: 0.007298\n",
      "[30] Training loss: 0.002173 | Validation loss: 0.002234\n",
      "[40] Training loss: 0.001434 | Validation loss: 0.005657\n",
      "[50] Training loss: 0.001656 | Validation loss: 0.005435\n",
      "[60] Training loss: 0.001413 | Validation loss: 0.001533\n",
      "[70] Training loss: 0.002200 | Validation loss: 0.003794\n",
      "[80] Training loss: 0.001304 | Validation loss: 0.000732\n",
      "[90] Training loss: 0.001067 | Validation loss: 0.001423\n",
      "Early stopping at epoch 99 (no improvement for 13 epochs).\n",
      "Loaded best model with validation loss: 0.000530\n",
      "[10] Training loss: 0.033595 | Validation loss: 0.025725\n",
      "[20] Training loss: 0.009324 | Validation loss: 0.005216\n",
      "[30] Training loss: 0.002063 | Validation loss: 0.001379\n",
      "[40] Training loss: 0.002096 | Validation loss: 0.001287\n",
      "[50] Training loss: 0.001817 | Validation loss: 0.003572\n",
      "Early stopping at epoch 54 (no improvement for 13 epochs).\n",
      "Loaded best model with validation loss: 0.001135\n",
      "[10] Training loss: 0.036379 | Validation loss: 0.034022\n",
      "[20] Training loss: 0.007414 | Validation loss: 0.004870\n",
      "[30] Training loss: 0.002264 | Validation loss: 0.001647\n",
      "[40] Training loss: 0.002655 | Validation loss: 0.002234\n",
      "[50] Training loss: 0.001910 | Validation loss: 0.001045\n",
      "[60] Training loss: 0.001608 | Validation loss: 0.000788\n",
      "[70] Training loss: 0.001723 | Validation loss: 0.000933\n",
      "[80] Training loss: 0.001106 | Validation loss: 0.000844\n",
      "[90] Training loss: 0.001289 | Validation loss: 0.003166\n",
      "Early stopping at epoch 92 (no improvement for 13 epochs).\n",
      "Loaded best model with validation loss: 0.000658\n",
      "[10] Training loss: 0.037466 | Validation loss: 0.040337\n",
      "[20] Training loss: 0.026117 | Validation loss: 0.031569\n",
      "[30] Training loss: 0.003237 | Validation loss: 0.003567\n",
      "[40] Training loss: 0.002573 | Validation loss: 0.002254\n",
      "[50] Training loss: 0.002290 | Validation loss: 0.001544\n",
      "[60] Training loss: 0.001557 | Validation loss: 0.001306\n",
      "[70] Training loss: 0.001554 | Validation loss: 0.002113\n",
      "[80] Training loss: 0.001393 | Validation loss: 0.001913\n",
      "[90] Training loss: 0.001337 | Validation loss: 0.001299\n",
      "Early stopping at epoch 91 (no improvement for 13 epochs).\n",
      "Loaded best model with validation loss: 0.001061\n",
      "[10] Training loss: 0.036165 | Validation loss: 0.040407\n",
      "[20] Training loss: 0.010830 | Validation loss: 0.014164\n",
      "[30] Training loss: 0.001877 | Validation loss: 0.003201\n",
      "[40] Training loss: 0.001597 | Validation loss: 0.003455\n",
      "[50] Training loss: 0.001701 | Validation loss: 0.003820\n",
      "[60] Training loss: 0.001592 | Validation loss: 0.000834\n",
      "[70] Training loss: 0.001077 | Validation loss: 0.000932\n",
      "Early stopping at epoch 71 (no improvement for 13 epochs).\n",
      "Loaded best model with validation loss: 0.000789\n",
      "[10] Training loss: 0.040753 | Validation loss: 0.051644\n",
      "[20] Training loss: 0.008288 | Validation loss: 0.010214\n",
      "[30] Training loss: 0.002859 | Validation loss: 0.005478\n",
      "[40] Training loss: 0.003655 | Validation loss: 0.005508\n",
      "[50] Training loss: 0.001815 | Validation loss: 0.001875\n",
      "[60] Training loss: 0.002294 | Validation loss: 0.005149\n",
      "Early stopping at epoch 63 (no improvement for 13 epochs).\n",
      "Loaded best model with validation loss: 0.001875\n",
      "[10] Training loss: 0.029984 | Validation loss: 0.037802\n",
      "[20] Training loss: 0.006325 | Validation loss: 0.004840\n",
      "[30] Training loss: 0.001733 | Validation loss: 0.001569\n",
      "[40] Training loss: 0.001446 | Validation loss: 0.001466\n",
      "[50] Training loss: 0.001684 | Validation loss: 0.002565\n",
      "[60] Training loss: 0.000889 | Validation loss: 0.001283\n",
      "Early stopping at epoch 69 (no improvement for 13 epochs).\n",
      "Loaded best model with validation loss: 0.000704\n",
      "[10] Training loss: 0.030142 | Validation loss: 0.050116\n",
      "[20] Training loss: 0.021318 | Validation loss: 0.018557\n",
      "[30] Training loss: 0.004546 | Validation loss: 0.003472\n",
      "[40] Training loss: 0.001724 | Validation loss: 0.001141\n",
      "[50] Training loss: 0.001565 | Validation loss: 0.001047\n",
      "[60] Training loss: 0.001620 | Validation loss: 0.001005\n",
      "[70] Training loss: 0.000934 | Validation loss: 0.000862\n",
      "[80] Training loss: 0.001713 | Validation loss: 0.000824\n",
      "Early stopping at epoch 84 (no improvement for 13 epochs).\n",
      "Loaded best model with validation loss: 0.000575\n",
      "[10] Training loss: 0.031246 | Validation loss: 0.037012\n",
      "[20] Training loss: 0.007852 | Validation loss: 0.008471\n",
      "[30] Training loss: 0.002323 | Validation loss: 0.001386\n",
      "[40] Training loss: 0.002163 | Validation loss: 0.003138\n",
      "[50] Training loss: 0.001397 | Validation loss: 0.001691\n",
      "[60] Training loss: 0.001345 | Validation loss: 0.000704\n",
      "[70] Training loss: 0.001095 | Validation loss: 0.000926\n",
      "[80] Training loss: 0.000923 | Validation loss: 0.000839\n",
      "Early stopping at epoch 88 (no improvement for 13 epochs).\n",
      "Loaded best model with validation loss: 0.000623\n",
      "[10] Training loss: 0.033111 | Validation loss: 0.029580\n",
      "[20] Training loss: 0.008829 | Validation loss: 0.008822\n",
      "[30] Training loss: 0.001718 | Validation loss: 0.001240\n",
      "[40] Training loss: 0.001574 | Validation loss: 0.002157\n",
      "[50] Training loss: 0.001197 | Validation loss: 0.001926\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      1\u001b[39m runner = FeedforwardNNRunner(  \n\u001b[32m      2\u001b[39m     experiment_config,\n\u001b[32m      3\u001b[39m     data_set_config,\n\u001b[32m   (...)\u001b[39m\u001b[32m      6\u001b[39m     fnn_config\n\u001b[32m      7\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m results = \u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/VU-8-THESIS-CODE/experiment_runners/base_runner.py:29\u001b[39m, in \u001b[36mBaseRunner.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     26\u001b[39m avg_std :\u001b[38;5;28mfloat\u001b[39m = \u001b[32m0.0\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.experiment_config.try_count):\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     curr_try_stats = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m     \u001b[38;5;66;03m# save current try statistics\u001b[39;00m\n\u001b[32m     32\u001b[39m     avg_min += curr_try_stats.min\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/VU-8-THESIS-CODE/experiment_runners/feedforward_nn_runner.py:68\u001b[39m, in \u001b[36mFeedforwardNNRunner._run_experiment\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28mself\u001b[39m.loss_func = nn.MSELoss()\n\u001b[32m     66\u001b[39m \u001b[38;5;28mself\u001b[39m.loss_optimization_func = optim.Adam(\u001b[38;5;28mself\u001b[39m.model.parameters(), lr = \u001b[38;5;28mself\u001b[39m.training_config.learning_rate)\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_data_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m predicted_scalars_tensor = \u001b[38;5;28mself\u001b[39m.__test(test_vectors_tensor, test_scalars_tensor)\n\u001b[32m     71\u001b[39m predicted_scalars = predicted_scalars_tensor.numpy()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/VU-8-THESIS-CODE/experiment_runners/feedforward_nn_runner.py:148\u001b[39m, in \u001b[36mFeedforwardNNRunner.__train\u001b[39m\u001b[34m(self, training_data_loader, validation_data_loader)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m    147\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m batch_vectors, batch_scalars \u001b[38;5;129;01min\u001b[39;00m validation_data_loader:\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m         predictions = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_vectors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m         loss = \u001b[38;5;28mself\u001b[39m.loss_func(predictions, batch_scalars)\n\u001b[32m    150\u001b[39m         epoch_validation_loss += loss.item() * batch_vectors.size(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/VU-8-THESIS-CODE/venv/lib64/python3.14/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/VU-8-THESIS-CODE/venv/lib64/python3.14/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/VU-8-THESIS-CODE/neural_network_models/feedforward_nn.py:25\u001b[39m, in \u001b[36mFeedforwardNN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     22\u001b[39m x = \u001b[38;5;28mself\u001b[39m.h1_activation_func(x)\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# output layer\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moutput_linear_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/VU-8-THESIS-CODE/venv/lib64/python3.14/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/VU-8-THESIS-CODE/venv/lib64/python3.14/site-packages/torch/nn/modules/module.py:1780\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m-> \u001b[39m\u001b[32m1780\u001b[39m     forward_call = (\u001b[38;5;28mself\u001b[39m._slow_forward \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_tracing_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.forward)\n\u001b[32m   1781\u001b[39m     \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m     \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m             \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m             \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "runner = FeedforwardNNRunner(  \n",
    "    experiment_config,\n",
    "    data_set_config,\n",
    "    noise_config,\n",
    "    training_config,\n",
    "    fnn_config\n",
    ")\n",
    "\n",
    "results = runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb706ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set absolute error min: 0.092178\n",
      "Test set absolute error max: 5.832191\n",
      "Test set absolute error mean: 2.322122\n",
      "Test set absolute error standard deviation: 1.492162\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test set absolute error min: {results.min:.6f}\")\n",
    "print(f\"Test set absolute error max: {results.max:.6f}\")\n",
    "print(f\"Test set absolute error mean: {results.mean:.6f}\")\n",
    "print(f\"Test set absolute error standard deviation: {results.std:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.14.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
